import pathlib
import pandas as pd
import sys
import subprocess

SCRIPTS_DIR = sys.path[0]
OUTPUT = config["output"]
EXPS = pd.DataFrame(config["experiments"])

with open(pathlib.Path(SCRIPTS_DIR, "resources", "logo.txt")) as f:
    print(f.read())

def set_name(files, data_type):
    filename = files.split('/')[-1]
    if data_type == 'protein':
        return filename # which is the foldername (e.g. input/mp1 -> mp1)
    if ',' in files:
        return filename.split(',')[0].split('_R')[0]
    return filename.split('.fa')[0]

for i in range(len(EXPS)):
    if pd.isnull(EXPS.iloc[i]['Name']) or EXPS.iloc[i]['Name'] == '':
        EXPS.iloc[i, EXPS.columns.get_loc('Name')] = set_name(
            EXPS.iloc[i]['Files'], EXPS.iloc[i]['Data type'])
    if not config['do_assembly']:
        EXPS.iloc[i]['Sample'] = EXPS.iloc[i]['Name']

pathlib.Path(f"{OUTPUT}").mkdir(parents=True, exist_ok=True)
EXPS.to_csv(f"{OUTPUT}/exps.tsv", sep = '\t', index = False)

mg_exps = EXPS[EXPS["Data type"] == 'dna']
mt_exps = EXPS[EXPS["Data type"] == 'mrna']
mp_exps = EXPS[EXPS["Data type"] == 'protein']

if len(mg_exps) == 0 and len(mt_exps) != 0:
    mg_exps = mt_exps
not_mp_exps = EXPS[EXPS["Data type"] != 'protein']

def all_input(wildcards):
    if config['do_assembly']:
        return (
            [f"{OUTPUT}/MOSCA_Protein_Report.xlsx",
            f"{OUTPUT}/MOSCA_Entry_Report.xlsx",
            f"{OUTPUT}/technical_report.tsv",
            f"{OUTPUT}/MOSCA_General_Report.tsv",
            f"{OUTPUT}/MOSCA_results.zip",
            f"{OUTPUT}/KEGG_maps/KEGGCharter_results.tsv"] +
            (expand("{output}/Binning/{sample}/checkm.tsv", output=OUTPUT, sample=set(EXPS['Sample'])) if
                config['do_binning'] else []) +
            expand("{output}/Annotation/{sample}/reCOGnizer_results.xlsx", output=OUTPUT,
                sample=set(EXPS['Sample'])) +
            (expand("{output}/Metaproteomics/{sample}/mp_quantification.tsv", output=OUTPUT,
                sample=set(mp_exps['Sample'])) if len(mp_exps) > 0 else [])
        )
    else:
        return f"{OUTPUT}/MOSCA_Entry_Counts_Report.xlsx"

def join_reads_input(wildcards):
    df = mg_exps[mg_exps['Sample'] == wildcards.sample].reset_index()
    return [f'{OUTPUT}/Preprocess/Trimmomatic/quality_trimmed_{df.iloc[i]["Name"]}{fr}.fq'
           for i in range(len(df))
           for fr in (['_forward_paired', '_reverse_paired'] if ',' in df.iloc[i]["Files"] else [''])]

def fastq2fasta_input(wildcards):
    return expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}.fq", output=OUTPUT,
        fr=(['_forward_paired', '_reverse_paired'] if EXPS["Files"].str.contains(',').tolist() else ''),
        name=wildcards.sample)

def gene_calling_input(wildcards):
    if config['do_assembly']:
        return expand("{output}/Assembly/{sample}/scaffolds.fasta", output=OUTPUT, sample=wildcards.sample)
    return expand(
        "{output}/Preprocess/piled_{name}.fasta", output=OUTPUT, name=wildcards.sample)

def upimapi_input(wildcards):
    if config['do_assembly']:
        return expand("{output}/Annotation/{sample}/aligned.blast", output=OUTPUT, sample=set(EXPS['Sample']))
    return expand("{output}/Annotation/{name}/aligned.blast", output=OUTPUT, name=set(EXPS['Name']))

rule all:
    input:
        all_input

rule preprocess:
    input:
        lambda wildcards: EXPS.loc[EXPS['Name'] == wildcards.name, 'Files'].iloc[0].split(',')
    output:
        expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{{name}}{fr}.fq", output=OUTPUT,
            fr=(['_forward_paired', '_reverse_paired'] if EXPS["Files"].str.contains(',').tolist() else ''))
    threads:
        config["threads"]
    params:
        reads = lambda wildcards: EXPS.loc[EXPS['Name'] == wildcards.name, 'Files'].iloc[0],
        resources_directory = config["resources_directory"],
        data_type = lambda wildcards: EXPS.loc[EXPS['Name'] == wildcards.name, 'Data type'].iloc[0],
        minlen = config["minimum_read_length"],
        avgqual = config["minimum_read_average_quality"]
    conda:
        "envs/mosca_pp.yaml"
    shell:
        "python {SCRIPTS_DIR}/preprocess.py -i {params.reads} -t {threads} -o {OUTPUT}/Preprocess "
        "-d {params.data_type} -rd {params.resources_directory} -n {wildcards.name} --minlen {params.minlen} "
        "--avgqual {params.avgqual}"

rule join_reads:
    input:
        join_reads_input
    output:
        expand("{output}/Preprocess/{{sample}}{fr}.fastq", output=OUTPUT,
            fr=(['_forward', '_reverse'] if EXPS["Files"].str.contains(',').tolist() else ''))
    threads:
        1
    run:
        for file in input:
            if 'forward' in file:
                shell("touch {output}/Preprocess/{wildcards.sample}_forward.fastq; cat {file} >> "
                      "{output}/Preprocess/{wildcards.sample}_forward.fastq", output=OUTPUT)
            elif 'reverse' in file:
                shell("touch {output}/Preprocess/{wildcards.sample}_reverse.fastq; cat {file} >> "
                      "{output}/Preprocess/{wildcards.sample}_reverse.fastq", output=OUTPUT)
            else:
                shell("touch {output}/Preprocess/{wildcards.sample}.fastq; cat {file} >> "
                      "{output}/Preprocess/{wildcards.sample}.fastq", output=OUTPUT)

rule assembly:
    input:
        expand("{output}/Preprocess/{{sample}}{fr}.fastq", output=OUTPUT,
            fr=(['_forward', '_reverse'] if EXPS["Files"].str.contains(',').tolist() else ''))
    output:
        expand("{output}/Assembly/{{sample}}/contigs.fasta", output=OUTPUT,
            sample=set(EXPS['Sample'])),
        expand("{output}/Assembly/{{sample}}/scaffolds.fasta", output=OUTPUT,
            sample=set(EXPS['Sample']))
    threads:
        config["threads"]
    params:
        assembler = config["assembler"],
        reads = ",".join(expand("{output}/Preprocess/{{sample}}{fr}.fastq", output=OUTPUT,
            fr=(['_forward', '_reverse'] if EXPS["Files"].str.contains(',').tolist() else ''))),
        max_memory = config["max_memory"]
    conda:
        "envs/mosca_ass.yaml"
    shell:
        "python {SCRIPTS_DIR}/assembly.py -r {params.reads} -t {threads} -a {params.assembler} -m {params.max_memory} "
        "-o {OUTPUT}/Assembly/{wildcards.sample}"

rule binning:
    input:
        reads = expand("{output}/Preprocess/{{sample}}{fr}.fastq", output=OUTPUT,
            fr=(['_forward', '_reverse'] if EXPS["Files"].str.contains(',').tolist() else '')),
        contigs = expand("{output}/Assembly/{{sample}}/scaffolds.fasta", output=OUTPUT)
    output:
        expand("{output}/Binning/{{sample}}/checkm.tsv", output=OUTPUT, sample=set(EXPS['Sample']))
    threads:
        config["threads"]
    params:
        markerset = config["markerset"],
        iterative_binning = ' --iterative' if config['do_iterative_binning'] else '',
        reads = lambda wildcards, input: input.reads[0] if len(input.reads) == 1 else ",".join(input.reads)
    conda:
        "envs/mosca_bin.yaml"
    shell:
        "python {SCRIPTS_DIR}/binning.py -c {input.contigs} -t {threads} -o {OUTPUT}/Binning/{wildcards.sample} "
        "-r {params.reads} -mset {params.markerset}{params.iterative_binning}"

rule fastq2fasta:
    input:
        fastq2fasta_input
    output:
        f"{OUTPUT}/Preprocess/piled_{{sample}}.fasta"
    threads:
        1
    shell:
        "cat {input} | paste - - - - | cut -f 1,2 | sed 's/^@/>/' | tr '\\t' '\\n' > {output}"

rule gene_calling:
    input:
        gene_calling_input
    output:
        expand("{output}/Annotation/{{sample}}/fgs.faa",output=OUTPUT),
        expand("{output}/Annotation/{{sample}}/fgs.ffn",output=OUTPUT)
    threads:
        config["threads"]
    params:
        error_model = "complete" if config['do_assembly'] else config["error_model"],
        complete = '1' if config['do_assembly'] else '0'
    conda:
        "envs/mosca_gencall.yaml"
    shell:
        'run_FragGeneScan.pl -thread={threads} -genome={input} -out={OUTPUT}/Annotation/{wildcards.sample}/fgs '
        '-complete={params.complete} -train=./{params.error_model}'

rule split_gene_calling:
    input:
        expand("{output}/Annotation/{{sample}}/fgs.faa", output=OUTPUT)
    output:
        expand("{output}/Annotation/{{sample}}/fgs.faa.split/fgs.part_{{part}}.faa", output=OUTPUT)
    threads:
        config["threads"]
    params:
        parts = config["split_gene_calling"]
    conda:
        "envs/mosca_seqkit.yaml"
    shell:
        'seqkit split --by-part {params.parts} -j {threads} {input}'

rule upimapi:
    input:
        expand("{output}/Annotation/{{sample}}/fgs.faa.split/fgs.part_{{part}}.faa", output=OUTPUT)
    output:
        expand("{output}/Annotation/{{sample}}/{{part}}/UPIMAPI_results.tsv", output=OUTPUT)
    threads:
        config["threads"]
    params:
        rd = config["resources_directory"],
        upimapi_database = config["upimapi_database"],
        taxids = f' --taxids {config["upimapi_taxids"]}' if config["upimapi_database"] == 'taxids' else '',
        max_target_seqs = config["upimapi_max_target_seqs"],
        cols = '&'.join(config['uniprot_columns']),
        check_db = '' if config['upimapi_check_db'] else ' --skip-db-check'
    conda:
        "envs/mosca_upimapi.yaml"
    shell:
        'upimapi.py -i {input} -t {threads} -o {OUTPUT}/Annotation/{wildcards.sample}/{wildcards.part} '
        '-rd {params.rd} -db {params.upimapi_database} -mts {params.max_target_seqs}{params.taxids} '
        '-cols "{params.cols}"{params.check_db}'

rule join_upimapi:
    input:
        expand("{output}/Annotation/{sample}/{part}/UPIMAPI_results.tsv", output=OUTPUT,
            part=[f'{"0" * (3 - len(str(i + 1)))}{i + 1}' for i in range(config["split_gene_calling"])],
            sample=set(EXPS["Sample"]))
    output:
        expand("{output}/Annotation/{sample}/UPIMAPI_results.tsv", output=OUTPUT,
            sample=set(EXPS["Sample"]))
    shell:
        "awk 'FNR==1{{if(NR!=1)next;}}{{print}}' {input} > {output}"

rule recognizer:
    input:
        orfs = expand("{output}/Annotation/{{sample}}/fgs.faa", output=OUTPUT),
        upimapi_results = expand("{output}/Annotation/{{sample}}/UPIMAPI_results.tsv", output=OUTPUT)
    output:
        expand("{output}/Annotation/{{sample}}/reCOGnizer_results.xlsx", output=OUTPUT)
    threads:
        config["threads"]
    params:
        resources_directory = config["resources_directory"],
        recognizer_databases = ','.join(config["recognizer_databases"]),
        download_cdd_resources = '' if not config['download_cdd_resources'] else ' -dr'
    conda:
        "envs/mosca_recog.yaml"
    shell:
        "recognizer.py -f {input.orfs} -t {threads} -o {OUTPUT}/Annotation/{wildcards.sample} "
        "-rd {params.resources_directory} -dbs {params.recognizer_databases} -sd "
        "--tax-file {input.upimapi_results} --protein-id-col Entry --tax-col 'Taxonomic lineage IDs (SPECIES)' "
        "--species-taxids --quiet{params.download_cdd_resources}"

rule quantification:
    input:
        expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}.fq", output=OUTPUT, name=not_mp_exps["Name"],
            fr=(['_forward_paired', '_reverse_paired'] if EXPS["Files"].str.contains(',').tolist() else '')),
        expand("{output}/Assembly/{sample}/contigs.fasta", output=OUTPUT, sample=set(EXPS["Sample"])),
        expand("{output}/Annotation/{sample}/fgs.ffn", output=OUTPUT, sample=set(EXPS["Sample"]))
    output:
        expand("{output}/Quantification/{sample}_mg.readcounts", output=OUTPUT, sample=set(mg_exps['Sample'])),
        expand("{output}/Quantification/{sample}_mt.readcounts", output=OUTPUT, sample=set(mt_exps['Sample']))
    threads:
        config["threads"]
    conda:
        "envs/mosca_quant.yaml"
    shell:
        "python {SCRIPTS_DIR}/quantification.py -o {OUTPUT} -e {OUTPUT}/exps.tsv -t {threads}"

rule metaproteomics:
    input:
        [directory(folder) for folder in mp_exps[mp_exps['Sample'] == (lambda wildcards: wildcards.sample)]['Files']],
        "{output}/Annotation/{sample}/UPIMAPI_results.tsv"
    output:
        "{output}/Metaproteomics/{sample}/spectracounts.tsv"
    threads:
        config["threads"]
    params:
        folders=lambda wildcards: ','.join(mp_exps[mp_exps['Sample'] == wildcards.sample]['Files'].tolist()),
        names=lambda wildcards: ','.join(mp_exps[mp_exps['Sample'] == wildcards.sample]['Name'].tolist()),
        contaminants_database=config["proteomics_contaminants_database"],
        protease=config["protease"] if config["protease_file"] == "" else config["protease_file"],
        max_memory=config["max_memory"],
        resources_directory=config["resources_directory"]
    conda:
        "envs/mosca_mp.yaml"
    shell:
        "python {SCRIPTS_DIR}/metaproteomics_analyser.py -sf {params.folders} -ns {params.names} -t {threads} "
        "-o {OUTPUT}/Metaproteomics/{wildcards.sample} -db {OUTPUT}/Annotation/{wildcards.sample}/fgs.faa "
        "-cdb {params.contaminants_database} --protease {params.protease} "
        "-ur {OUTPUT}/Annotation/{wildcards.sample}/UPIMAPI_results.tsv -mmem {params.max_memory} "
        "-rd {params.resources_directory}"

rule protein_report:
    input:
        expand("{output}/Annotation/{sample}/UPIMAPI_results.tsv", output=OUTPUT, sample=set(EXPS['Sample'])),
        expand("{output}/Annotation/{sample}/reCOGnizer_results.xlsx", output=OUTPUT, sample=set(EXPS["Sample"])),
        expand("{output}/Quantification/{name}.readcounts", output=OUTPUT, name=set(mt_exps['Name'])),
        expand("{output}/Metaproteomics/{sample}/spectracounts.tsv", output=OUTPUT, sample=set(mp_exps['Sample']))
    output:
        f"{OUTPUT}/MOSCA_Protein_Report.xlsx",
        expand("{output}/Quantification/{sample}/mt.readcounts", output=OUTPUT, sample=set(mt_exps['Sample'])),
        expand("{output}/Metaproteomics/{sample}/mp.spectracounts", output=OUTPUT, sample=set(mp_exps['Sample']))
    threads:
        1
    conda:
        "envs/mosca_reports.yaml"
    shell:
        "python {SCRIPTS_DIR}/main_reports.py -o {OUTPUT} -e {OUTPUT}/exps.tsv --protein-report"

rule normalization:
    input:
        expand("{output}/Quantification/{sample}/mt.readcounts", output=OUTPUT, sample=set(mt_exps['Sample'])),
        expand("{output}/Metaproteomics/{sample}/mp.spectracounts", output=OUTPUT, sample=set(mp_exps['Sample']))
    output:
        expand("{output}/Quantification/{sample}/mt_normalized.tsv",output=OUTPUT,sample=set(mt_exps['Sample'])),
        expand("{output}/Metaproteomics/{sample}/mp_normalized.tsv",output=OUTPUT,sample=set(mp_exps['Sample']))
    threads:
        1
    params:
        nm = config["normalization_method"]
    conda:
        "envs/mosca_norm.yaml"
    shell:
        "Rscript {SCRIPTS_DIR}/normalization.R -c {input} -m {params.nm} -o {output}"

rule entry_report:
    input:
        f"{OUTPUT}/MOSCA_Protein_Report.xlsx",
        expand("{output}/Quantification/{sample}/mt_normalized.tsv", output=OUTPUT, sample=set(mt_exps['Sample'])),
        expand("{output}/Metaproteomics/{sample}/mp_normalized.tsv", output=OUTPUT, sample=set(mp_exps['Sample']))
    output:
        f"{OUTPUT}/MOSCA_Entry_Report.xlsx"
    threads:
        1
    conda:
        "envs/mosca_reports.yaml"
    shell:
        "python {SCRIPTS_DIR}/main_reports.py -o {OUTPUT} -e {OUTPUT}/exps.tsv --entry-report"

'''
rule entry_count:
    input:
        uniprotinfo=f"{OUTPUT}/Annotation/uniprotinfo.tsv",
        blasts=expand("{output}/Annotation/{name}/aligned.blast",output=OUTPUT, name=mg_exps['Name'].tolist())
    output:
        f"{OUTPUT}/MOSCA_Entry_Counts_Report.xlsx",
        f"{OUTPUT}/Quantification/expression_matrix.tsv"
    threads:
        1
    run:
        uniprotinfo = pd.read_csv(input.uniprotinfo[0], sep='\t')
        result = pd.DataFrame(columns=['sseqid'])
        i = 1
        names = []
        for blast in input.blasts:
            name = blast.split('/')[-2]
            print(f'[{i}/{len(input.blasts)}] Quantifying entries for: {blast}')
            data = parse_blast(blast).groupby('sseqid').size().reset_index(name=name)
            data['sseqid'] = [ide.split('|')[1] if ide != '*' else ide for ide in data['sseqid']]
            result = pd.merge(result, data, on='sseqid', how='outer')
            i += 1
            names.append(name)
        result.columns = ['Entry'] + result.columns.to_list()[1:]
        print(f'Merging entry counts with info at {input.uniprotinfo[0]}')
        result = pd.merge(result, uniprotinfo, on='Entry', how='left')
        multi_sheet_excel(f"{OUTPUT}/MOSCA_Entry_Counts_Report.xlsx", result, sheet_name='Sheet')
        result.to_csv(f"{OUTPUT}/MOSCA_Entry_Counts_Report.tsv", index=False, sep='\t')
        result[['Entry'] + names].to_csv(f"{OUTPUT}/Quantification/expression_matrix.tsv",
                                         sep='\t', index=False)
'''

rule mp_differential_expression:
    input:
        "{output}/Metaproteomics/{sample}/spectracounts.tsv"
    output:
        "{output}/Metaproteomics/{sample}/mp_quantification.tsv"
    threads:
        1
    params:
        conditions=','.join(EXPS[EXPS['Data type'] == 'protein']['Condition'].tolist())
    conda:
        "envs/mosca_mp_de.yaml"
    shell:
        "Rscript {SCRIPTS_DIR}/proteomics_postprocessing.R -m {input} "
        "-o {wildcards.output}/Metaproteomics/{wildcards.sample} -c {params.conditions}"

rule de_analysis:
    input:
        expand("{output}/Quantification/{sample}/mt.readcounts", output=OUTPUT, sample=set(mt_exps['Sample'])),
        expand("{output}/Metaproteomics/{sample}/mp_normalized.tsv", output=OUTPUT, sample=set(mp_exps['Sample']))
    output:
        expand("{output}/Quantification/{{sample}}/condition_treated_results.tsv", output=OUTPUT)
    threads:
        15
    params:
        conditions = ",".join(mt_exps['Condition'].tolist()),
        minimum_fold_change=config["minimum_differential_expression"],
        fdr=config["false_discovery_rate"],
        # "params.type_of_data" is "rna_seq" if "readcounts" is in the input otherwise it is "proteomics"
        type_of_data = "rna_seq" if (lambda wildcards: wildcards.sample) in mt_exps['Sample'] else "proteomics"
    conda:
        "envs/mosca_mt_de.yaml"
    shell:
        "Rscript {SCRIPTS_DIR}/de_analysis.R --counts {input} --conditions {params.conditions} "
        "--output {OUTPUT}/Quantification/{wildcards.sample} --foldchange {params.minimum_fold_change} "
        "--fdr {params.fdr} --data-type {params.type_of_data}"

rule keggcharter:
    input:
        f"{OUTPUT}/MOSCA_Entry_Report.xlsx"
    output:
        f"{OUTPUT}/KEGG_maps/KEGGCharter_results.tsv"
    threads:
        1
    params:
        outdir=f"{OUTPUT}/KEGG_maps",
        mg_cols= ','.join(mg_exps['Name'].tolist()),
        resources_directory=config["resources_directory"],
        metabolic_maps=f" -mm {','.join(config['keggcharter_maps']) if len(config['keggcharter_maps']) > 0 else ''}",
        exp_cols=f" -tcol {','.join(mt_exps['Name'].tolist())}" if len(mt_exps) > 0 else '',
        taxa_level=config["keggcharter_taxa_level"],
        number_of_taxa=config["keggcharter_number_of_taxa"]
    conda:
        "envs/mosca_kegg.yaml"
    shell:
        "keggcharter.py -f {input} -o {params.outdir} -gcol {params.mg_cols}{params.exp_cols} "
        "-tc 'Taxonomic lineage ({params.taxa_level})' -not {params.number_of_taxa} -keggc KEGG "
        "-rd {params.resources_directory}{params.metabolic_maps}"


rule report:
    input:
        f"{OUTPUT}/MOSCA_Protein_Report.xlsx",
        f"{OUTPUT}/MOSCA_Entry_Report.xlsx",
        (expand("{output}/Quantification/{sample}/condition_treated_results.tsv", output=OUTPUT,
            sample=set(mt_exps['Sample'].tolist())) if len(mt_exps) > 0 else [])
    output:
        f"{OUTPUT}/technical_report.tsv",
        f"{OUTPUT}/MOSCA_General_Report.tsv",
        f"{OUTPUT}/MOSCA_results.zip"
    threads:
        1
    conda:
        "envs/mosca_final.yaml"
    shell:
        "python {SCRIPTS_DIR}/report.py -o {OUTPUT}"
