import pandas as pd
import shutil
import sys
from mosca_tools import parse_blast, multi_sheet_excel

scripts_dir = sys.path[0]

if config["experiments"].endswith('.xlsx'):
    experiments = pd.read_excel(config["experiments"])
    input_format = 'excel'
else:
    experiments = pd.read_csv(config["experiments"], sep = '\t')
    input_format = 'tsv'

def set_name(files, data_type):
    filename = files.split('/')[-1]
    if data_type == 'protein':
        return '_'.join(filename.split('_')[:-1])
    if ',' in files:
        return filename.split(',')[0].split('_R')[0]
    return filename.split('.fa')[0]

for i in range(len(experiments)):
    if pd.isnull(experiments.iloc[i]['Name']):
        experiments.iloc[i]['Name'] = set_name(experiments.iloc[i]['Files'], experiments.iloc[i]['Data type'])
    if not config['do_assembly']:
        experiments.iloc[i]['Sample'] = experiments.iloc[i]['Name']
'''
if input_format == 'excel':
    experiments.to_excel(config["experiments"], index = False)
else:
    experiments.to_csv(config["experiments"], sep = '\t', index = False)
'''

name2sample = {experiments.iloc[i]['Name'] : experiments.iloc[i]['Sample'] for i in range(len(experiments))}
mg_experiments = experiments[experiments["Data type"] == 'dna']
mt_experiments = experiments[experiments["Data type"] == 'mrna']

'''
sample2mgname = dict()
for row in mg_experiments.iterrows():
    if row[1].loc['Sample'] in sample2mgname.keys():
        sample2mgname[row[1].loc['Sample']].append(row[1].loc['Name'])
    else:
        sample2mgname[row[1].loc['Sample']] = [row[1].loc['Name']]
'''

def all_input(wildcards):
    if config['do_assembly']:
        return (
            expand("{output}/MOSCA_Protein_Report.xlsx", output=config["output"]) +
            expand("{output}/MOSCA_Entry_Report.xlsx", output=config["output"]) +
            expand("{output}/technical_report.tsv", output=config["output"]) +
            expand("{output}/MOSCA_General_Report.xlsx", output=config["output"]) +
            expand("{output}/MOSCA_results.zip", output=config["output"]) +
            expand("{output}/Binning/{sample}/checkm.tsv", output=config["output"], sample=set(experiments['Sample']))
        )
    else:
        return (
            "{}/MOSCA_Entry_Counts_Report.xlsx".format(config["output"])
        )

def preprocess_input(wildcards):
    # get first value (in case multiple) and split on commas
    return experiments.loc[experiments['Name'] == wildcards.name, 'Files'].iloc[0].split(',')

def join_reads_input(wildcards):
    df = mg_experiments.loc[mg_experiments['Sample'] == wildcards.sample]
    files = df['Files'].tolist()
    names = df['Name'].tolist()
    return ['{}/Preprocess/Trimmomatic/quality_trimmed_{}{}.fq'.format(config["output"], name, fr) for name in names
        for file_list in files for fr in (['_forward_paired', '_reverse_paired'] if ',' in file_list else [''])]

def fastq2fasta_input(wildcards):
    return expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}.fq", output=config["output"],
        fr=(['_forward_paired', '_reverse_paired'] if experiments["Files"].str.contains(',').tolist() else ''),
        name = wildcards.sample)

def annotation_input(wildcards):
    if config['do_assembly']:
        return expand("{output}/Assembly/{sample}/contigs.fasta", output = config["output"],
            sample = set(experiments['Sample']))
    return expand("{output}/Preprocess/piled_{name}.fasta", output = config["output"],
        name = wildcards.sample)

def upimapi_input(wildcards):
    if config['do_assembly']:
        return expand("{output}/Annotation/{sample}/aligned.blast",output=config["output"],
            sample = set(experiments['Sample']))
    return expand("{output}/Annotation/{name}/aligned.blast",output=config["output"],
        name = set(experiments['Name']))

rule all:
    input:
        all_input

rule preprocess:
    input:
        preprocess_input
    output:
        expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{{name}}{fr}.fq", output = config["output"],
            fr = (['_forward_paired', '_reverse_paired'] if experiments["Files"].str.contains(',').tolist() else ''))
    threads:
        config["threads"]
    run:
        shell("python {scripts_dir}/preprocess.py -i {reads} -t {threads} -o {output}/Preprocess -adaptdir "
              "{resources_directory}/adapters -rrnadbs {resources_directory}/rRNA_databases -d {data_type} -rd "
              "{resources_directory} -n {wildcards.name} --minlen {minlen} --avgqual {avgqual}",
            output = config["output"], reads = ",".join(input), resources_directory = config["resources_directory"],
            data_type = experiments.loc[experiments['Name'] == wildcards.name]["Data type"].iloc[0],
            minlen = config["minimum_read_length"], avgqual = config["minimum_read_average_quality"])

rule join_reads:
    input:
        join_reads_input
    output:
        expand("{output}/Preprocess/{{sample}}{fr}.fastq", output = config["output"],
            fr = (['_forward', '_reverse'] if experiments["Files"].str.contains(',').tolist() else ''))
    run:
        for file in input:
            if 'forward' in file:
                shell("touch {output}/Preprocess/{wildcards.sample}_forward.fastq; cat {file} >> "
                      "{output}/Preprocess/{wildcards.sample}_forward.fastq", output = config["output"])
            elif 'reverse' in file:
                shell("touch {output}/Preprocess/{wildcards.sample}_reverse.fastq; cat {file} >> "
                      "{output}/Preprocess/{wildcards.sample}_reverse.fastq", output = config["output"])
            else:
                shell("touch {output}/Preprocess/{wildcards.sample}.fastq; cat {file} >> "
                      "{output}/Preprocess/{wildcards.sample}.fastq", output = config["output"])

rule assembly:
    input:
        expand("{output}/Preprocess/{sample}{fr}.fastq", output = config["output"], sample = set(experiments['Sample']),
            fr = (['_forward', '_reverse'] if experiments["Files"].str.contains(',').tolist() else ''))
    output:
        expand("{output}/Assembly/{sample}/contigs.fasta", output = config["output"],
            sample = set(experiments['Sample']))
    threads:
        config["threads"]
    run:
        reads = ",".join(input)
        shell("python {scripts_dir}/assembly.py -r {reads} -t {threads} -o {output}/Assembly/{sample} -a {assembler} "
              "-m {max_memory}",
            output = config["output"], sample = set(experiments['Sample']), assembler = config["assembler"],
            max_memory = config["max_memory"])

rule binning:
    input:
        reads = expand("{output}/Preprocess/{sample}{fr}.fastq", output = config["output"],
            sample = set(experiments['Sample']),
            fr = (['_forward', '_reverse'] if experiments["Files"].str.contains(',').tolist() else '')),
        contigs = expand("{output}/Assembly/{sample}/contigs.fasta", output = config["output"],
            sample = set(experiments['Sample']))
    output:
        expand("{output}/Binning/{sample}/checkm.tsv", output = config["output"], sample = set(experiments['Sample']))
    threads:
        config["threads"]
    run:
        reads = ",".join(input.reads)
        shell("python {scripts_dir}/binning.py -c {input.contigs} -t {threads} -o {output}/Binning/{sample} -r {reads} "
              "-mset {markerset}",
            output = config["output"], markerset = config["markerset"], sample = set(experiments['Sample']))

rule fastq2fasta:
    input:
        fastq2fasta_input
    output:
        expand("{output}/Preprocess/piled_{{sample}}.fasta", output=config["output"])
    threads:
        1
    shell:
        "cat {input} | paste - - - - | cut -f 1,2 | sed 's/^@/>/' | tr '\\t' '\\n' > {output}"

rule annotation:
    input:
        annotation_input
    output:
        expand("{output}/Annotation/{{sample}}/fgs.faa", output = config["output"]),
        expand("{output}/Annotation/{{sample}}/aligned.blast", output = config["output"])
    threads:
        config["threads"]
    run:
        if not config['do_assembly']:
            input = ",".join(input)
        shell("python {scripts_dir}/annotation.py -i {input} -t {threads} -o {output}/Annotation/{wildcards.sample} -em "
              "{error_model} -db {diamond_database} -mts {diamond_max_target_seqs}{download_uniprot}{assembled}",
            output = config["output"], error_model = config["error_model"],
            diamond_database = config["diamond_database"], diamond_max_target_seqs = config["diamond_max_target_seqs"],
            download_uniprot = ' --download-uniprot' if config["download_uniprot"] else '',
            assembled = ' --assembled' if config['do_assembly'] else '')

rule upimapi:
    input:
        upimapi_input
    output:
        expand("{output}/Annotation/uniprotinfo.tsv", output = config["output"]),
    threads:
        1
    run:
        if config["do_assembly"]:
            shell("upimapi.py -i {input} -o {output}/Annotation/uniprotinfo --blast --full-id", output = config["output"])
        else:
            for blast in input:
                shell("upimapi.py -i {blast} -o {output}/Annotation/uniprotinfo --blast --full-id",
                    output=config["output"])

rule recognizer:
    input:
        expand("{output}/Annotation/{sample}/fgs.faa", output = config["output"], sample = set(experiments["Sample"]))
    output:
        expand("{output}/Annotation/{sample}/reCOGnizer_results.xlsx", output = config["output"],
            sample = set(experiments["Sample"]))
    threads:
        config["threads"] - 1
    run:
        shell("recognizer.py -f {input} -t {threads} -o {output}/Annotation/{sample} -rd {resources_directory} "
              "--remove-spaces{download_resources}",
            output = config["output"], sample = set(experiments["Sample"]),
            resources_directory = config["resources_directory"],
            download_resources = '' if not config['download_cdd'] else ' --download-resources')

rule quantification_analysis:
    input:
        expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}.fq", output = config["output"],
            name = experiments["Name"],
            fr = (['_forward_paired', '_reverse_paired'] if experiments["Files"].str.contains(',').tolist() else '')),
        expand("{output}/Assembly/{sample}/contigs.fasta", output = config["output"],
            sample = set(experiments["Sample"])),
        expand("{output}/Annotation/{sample}/aligned.blast", output = config["output"],
            sample = set(experiments["Sample"]))
    output:
        expand("{output}/Metatranscriptomics/{name}.readcounts", output = config["output"],
            name = set(mt_experiments['Name'])),
        expand("{output}/Annotation/{name}.readcounts", output = config["output"],
            name = set(mg_experiments['Name']))
    threads:
        config["threads"]
    run:
        shell("python {scripts_dir}/quantification_analyser.py -e {experiments} -t {threads} -o {output} -if "
              "{input_format}",
              experiments = config["experiments"], output = config["output"])

rule metaphlan:
    input:
        expand("{output}/Preprocess/Trimmomatic/quality_trimmed_{name}{fr}.fq", output = config["output"],
            name = mg_experiments["Name"],
            fr = (['_forward_paired', '_reverse_paired'] if experiments["Files"].str.contains(',').tolist() else ''))
    output:
        expand("{output}/Taxonomy/{sample}_profiled_metagenome.txt", output = config["output"],
            sample = set(experiments["Sample"]))
    threads:
        config["threads"]
    run:
        reads = ",".join(input)
        shell("metaphlan {reads} --bowtie2out {output}/Taxonomy/{sample}_mg.bowtie2.bz2 --nproc {threads} --input_type "
              "fastq",
              output = config["output"], sample = set(experiments["Sample"]))
        shell("metaphlan {output}/Taxonomy/{sample}_mg.bowtie2.bz2 --nproc {threads} --input_type bowtie2out -o "
              "{output}/Taxonomy/{sample}_profiled_metagenome.txt",
              output = config["output"], sample = set(experiments["Sample"]))

rule join_information:
    input:
        expand("{output}/Annotation/uniprotinfo.tsv", output = config["output"]),
        expand("{output}/Annotation/{sample}/aligned.blast", output = config["output"],
            sample = set(experiments['Sample'])),
        expand("{output}/Annotation/{sample}/reCOGnizer_results.xlsx", output = config["output"],
            sample = set(experiments["Sample"])),
        expand("{output}/Metatranscriptomics/{name}.readcounts", output = config["output"],
            name = set(mt_experiments['Name'])),
        expand("{output}/Annotation/{name}.readcounts", output = config["output"],
            name = set(mg_experiments['Name']))
    output:
        expand("{output}/MOSCA_Protein_Report.xlsx", output = config["output"]),
        expand("{output}/MOSCA_Entry_Report.xlsx", output = config["output"]),
        expand("{output}/Metatranscriptomics/expression_matrix.tsv", output = config["output"])
    threads:
        config["threads"] - 2
    run:
        shell("python {scripts_dir}/join_information.py -e {experiments} -t {threads} -o {output} -if {input_format} "
              "-nm {normalization_method}",
              experiments = config["experiments"], output = config["output"],
              normalization_method = config["normalization_method"])

rule entry_count:
    input:
        uniprotinfo=expand("{output}/Annotation/uniprotinfo.tsv",output=config["output"]),
        blasts=expand("{output}/Annotation/{name}/aligned.blast",output=config["output"], name=set(experiments['Name']))
    output:
        "{}/MOSCA_Entry_Counts_Report.xlsx".format(config["output"])
    threads:
        1
    run:
        uniprotinfo = pd.read_csv(input.uniprotinfo[0], sep='\t')
        result = pd.DataFrame(columns=['sseqid'])
        i = 1
        for blast in input.blasts:
            print('[{}/{}] Quantifying entries for: {}'.format(i, len(input.blasts), blast))
            data = parse_blast(blast).groupby('sseqid').size().reset_index(name=blast.split('/')[-2])
            data['sseqid'] = [ide.split('|')[1] if ide != '*' else ide for ide in data['sseqid']]
            result = pd.merge(result, data, on='sseqid', how='outer')
            i += 1
        result.columns = ['Entry'] + result.columns.to_list()[1:]
        print('Merging entry counts with info at {}'.format(input.uniprotinfo[0]))
        result = pd.merge(result, uniprotinfo, on='Entry', how='left')
        multi_sheet_excel("{}/MOSCA_Entry_Counts_Report.xlsx".format(config["output"]), result, sheet_name='Sheet')
        result.to_csv("{}/MOSCA_Entry_Counts_Report.tsv".format(config["output"]), index=False, sep='\t')

rule differential_expression:
    input:
        expand("{output}/Metatranscriptomics/expression_matrix.tsv", output = config["output"])
    output:
        expand("{output}/Metatranscriptomics/gene_expression.jpeg", output = config["output"]),
        expand("{output}/Metatranscriptomics/sample_distances.jpeg", output = config["output"]),
        expand("{output}/Metatranscriptomics/condition_treated_results.csv", output = config["output"])
    threads:
        1
    run:
        conditions = ",".join(map(str, mt_experiments['Condition'].tolist()))
        shell("{scripts_dir}/../../../bin/Rscript {scripts_dir}/de_analysis.R --readcounts {input} --conditions "
              "{conditions} --output {output}/Metatranscriptomics",
            conditions = conditions, output = config["output"])

rule keggcharter:
    input:
        expand("{output}/MOSCA_Entry_Report.xlsx", output = config["output"])
    output:
        expand("{output}/KEGG_maps/KEGGCharter_results.xlsx", output = config["output"])
    threads:
        1
    run:
        shell("kegg_charter.py -f {input} -o {output}/KEGG_maps{metabolic_maps} -gcol {mg_cols} -tcol {exp_cols} -tc "
              "'Taxonomic lineage ({taxa_level})' -not {number_of_taxa} -keggc 'Cross-reference (KEGG)'",
              output = config["output"], mg_cols = ','.join(mg_experiments['Name'].tolist()),
              metabolic_maps = " -mm {}".format(','.join(config["keggcharter_maps"])) if
              len(config["keggcharter_maps"]) > 0 else '',
              exp_cols = ','.join(mt_experiments['Name'].tolist()), taxa_level = config["keggcharter_taxa_level"],
              number_of_taxa = config["keggcharter_number_of_taxa"])

        shutil.copyfile('{}/KEGGCharter_results.xlsx'.format(config["output"]),
                        '{}/MOSCA_Entry_Report.xlsx'.format(config["output"]))

rule report:
    input:
        expand("{output}/MOSCA_Protein_Report.xlsx", output = config["output"]),
        expand("{output}/Metatranscriptomics/condition_treated_results.csv", output = config["output"])
    output:
        expand("{output}/technical_report.tsv", output = config["output"]),
        expand("{output}/MOSCA_General_Report.xlsx", output = config["output"]),
        expand("{output}/MOSCA_results.zip", output = config["output"])
    threads:
        1
    run:
        shell("python {scripts_dir}/report.py -e {experiments} -o {output} -ldir {reporter_lists} -if {input_format}",
              experiments = config["experiments"], output = config["output"],
              reporter_lists = '{}/../resources'.format(scripts_dir))